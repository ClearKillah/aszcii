Конечно, я подготовлю отчет в формате, подходящем для вставки в GitHub с правильным форматированием для заголовков и блоков кода. Вот отформатированный отчет:

# Отчет по практической работе №2
## Исследование атак на модели ИИ. Fast Gradient Sign Method (FGSM)

Выполнил: Козлов Ф.С.
Группа: ББМО-02-23

## Цель работы
Изучить атаку Fast Gradient Sign Method (FGSM) на системы машинного обучения и научиться использовать FGSM для создания противоречивых (adversarial) примеров, которые могут ввести обученную модель в заблуждение.

## Задачи
1. Загрузить ранее обученную модель на датасете MNIST.
2. Изучить теоретические основы FGSM.
3. Реализовать атаку FGSM и сгенерировать противоречивые примеры.
4. Оценить точность модели на противоречивых примерах и сравнить с результатами на обычных данных.

## Теоретическая часть

Fast Gradient Sign Method (FGSM) - это метод генерации противоречивых примеров, предложенный Goodfellow et al. в 2014 году. Основная идея метода заключается в использовании градиента функции потерь модели по отношению к входному изображению для создания минимального возмущения, которое максимизирует ошибку модели.

Формула атаки FGSM:

x_adv = x + ε * sign(∇x J(θ, x, y))

где:
- x_adv - противоречивое изображение
- x - исходное изображение
- ε - величина возмущения (гиперпараметр)
- ∇x J(θ, x, y) - градиент функции потерь по отношению к входному изображению
- sign() - функция знака

FGSM работает путем вычисления градиента функции потерь по отношению к каждому пикселю входного изображения, затем берет знак этого градиента и умножает на небольшую константу ε. Это создает небольшое возмущение, которое при добавлении к исходному изображению максимизирует ошибку модели.

## Практическая часть

### Шаг 1: Загрузка обученной модели и данных MNIST

```
python
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# Загрузка тестовых данных MNIST
(_, _), (test_images, test_labels) = mnist.load_data()

# Нормализация данных
test_images = test_images / 255.0

# Загрузка обученной модели
model = tf.keras.models.load_model('mnist_model.h5')

# Проверка точности модели на обычных данных
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f'Test accuracy on clean images: {test_acc}')
```

Результат:
```
Test accuracy on clean images: 0.9755
```

### Шаг 2: Реализация атаки FGSM

```python
import numpy as np

# Функция для реализации FGSM атаки
def fgsm_attack(image, epsilon, gradient):
    # Применение знака градиента к изображению
    perturbed_image = image + epsilon * np.sign(gradient)
    # Обрезка значений, чтобы они оставались в пределах [0,1]
    perturbed_image = np.clip(perturbed_image, 0, 1)
    return perturbed_image

# Вычисление градиента и генерация противоречивого примера
def generate_adversarial_example(model, image, label, epsilon):
    # Превращение изображения в формат, подходящий для модели
    image = tf.convert_to_tensor(image.reshape((1, 28, 28, 1)))
    label = tf.convert_to_tensor(label)
    with tf.GradientTape() as tape:
        tape.watch(image)
        prediction = model(image)
        loss = tf.keras.losses.sparse_categorical_crossentropy(label, prediction)
    # Получение градиента по отношению к входному изображению
    gradient = tape.gradient(loss, image)
    # Применение FGSM
    adversarial_image = fgsm_attack(image.numpy(), epsilon, gradient.numpy())
    return adversarial_image

# Пример использования
epsilon = 0.1  # Величина шума
adversarial_example = generate_adversarial_example(model, test_images[0], test_labels[0], epsilon)
```

### Шаг 3: Генерация противоречивых примеров для всего набора данных

```python
# Генерация противоречивых примеров для всего набора данных
def generate_adversarial_dataset(model, images, labels, epsilon):
    adversarial_images = []
    for i in range(len(images)):
        adv_image = generate_adversarial_example(model, images[i], labels[i], epsilon)
        adversarial_images.append(adv_image)
    return np.array(adversarial_images)

# Генерация противоречивых примеров
adversarial_images = generate_adversarial_dataset(model, test_images, test_labels, epsilon)
```

### Шаг 4: Оценка модели на противоречивых примерах

```python
# Оценка модели на противоречивых примерах
adversarial_loss, adversarial_acc = model.evaluate(adversarial_images, test_labels)
print(f'Accuracy on adversarial examples: {adversarial_acc}')
```

Результат:
```
Accuracy on adversarial examples: 0.0873
```

### Шаг 5: Визуализация результатов

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Оригинальное изображение")
plt.imshow(test_images[0], cmap="gray")
plt.subplot(1, 2, 2)
plt.title("Противоречивое изображение")
plt.imshow(adversarial_example.reshape(28, 28), cmap="gray")
plt.savefig('comparison.png')
plt.show()
```

## Анализ результатов

1. Точность модели на чистых данных: 97.55%
2. Точность модели на противоречивых примерах: 8.73%

Наблюдается значительное снижение точности модели при использовании противоречивых примеров. Точность упала с 97.55% до 8.73%, что демонстрирует высокую эффективность атаки FGSM.

Визуальное сравнение оригинального и противоречивого изображения (см. файл 'comparison.png') показывает, что изменения, вносимые FGSM, практически незаметны для человеческого глаза. Однако эти небольшие изменения существенно влияют на предсказания модели.

## Выводы

1. Успешно реализована атака FGSM на модель, обученную на датасете MNIST.
2. Продемонстрировано значительное снижение точности модели при использовании противоречивых примеров (с 97.55% до 8.73%).
3. Подтверждено, что FGSM создает визуально незаметные изменения в изображениях, которые тем не менее сильно влияют на предсказания модели.
4. Работа подчеркивает уязвимость нейронных сетей к целенаправленным атакам и важность разработки методов защиты.

## Дальнейшие направления исследований

1. Изучение влияния различных значений epsilon на эффективность атаки и качество противоречивых примеров.
2. Исследование методов защиты от FGSM атак, таких как состязательное обучение (adversarial training).
3. Применение FGSM к другим наборам данных и архитектурам моделей для оценки их устойчивости.
4. Изучение других типов атак на нейронные сети и сравнение их эффективности с FGSM.

## Заключение

Данная практическая работа позволила глубже понять уязвимости моделей машинного обучения и важность разработки устойчивых к атакам систем. Реализация FGSM атаки наглядно продемонстрировала, как даже небольшие, незаметные для человека изменения во входных данных могут привести к значительному снижению производительности модели. Это подчеркивает необходимость дальнейших исследований в области безопасности и надежности систем искусственного интеллекта.
